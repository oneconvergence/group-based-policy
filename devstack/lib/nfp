#!/bin/bash

NFPSERVICE_DIR=$DEST/gbp
TOP_DIR=$PWD
DISK_IMAGE_DIR=$DEST/gbp/gbpservice/tests/contrib

NEUTRON_CONF_DIR=/etc/neutron
NEUTRON_CONF=$NEUTRON_CONF_DIR/neutron.conf

NFP_CONF_DIR=/etc/nfp
echo "TOP-DIR-NFP : $PWD"

function prepare_nfp_image_builder {
    sudo -H -E pip install -r $DISK_IMAGE_DIR/diskimage-create/requirements.txt
    sudo apt-get install -y --force-yes qemu-utils
}

function create_nfp_image {
    source $TOP_DIR/openrc neutron service
    unset OS_USER_DOMAIN_ID
    unset OS_PROJECT_DOMAIN_ID

    if [[ $DEVSTACK_MODE = base ]]; then

        RefConfiguratorQcow2ImageName=reference_configurator_image
        echo "Building Image: $RefConfiguratorQcow2ImageName"
        sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/ref_configurator_conf.json
        RefConfiguratorQcow2Image=$(cat /tmp/image_path)
        echo "Uploading Image: $RefConfiguratorQcow2ImageName"
        glance image-create --name $RefConfiguratorQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $RefConfiguratorQcow2Image
        openstack --os-cloud=devstack-admin flavor create --ram 512 --disk 3 --vcpus 1 m1.nfp-tiny

    else

        if [[ $DEVSTACK_MODE = enterprise ]]; then

            VisibilityQcow2ImageName=visibility
            if [[ $VisibilityQcow2Image = build ]]; then
                echo "Building Image: $VisibilityQcow2ImageName"
                sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/visibility_conf.json
                VisibilityQcow2Image=$(cat /tmp/image_path)
            fi
            echo "Uploading Image: $VisibilityQcow2ImageName"
            glance image-create --name $VisibilityQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $VisibilityQcow2Image

        else

            ConfiguratorQcow2ImageName=configurator
            if [[ $ConfiguratorQcow2Image = build ]]; then
                echo "Building Image: $ConfiguratorQcow2ImageName"
                sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/configurator_conf.json
                ConfiguratorQcow2Image=$(cat /tmp/image_path)
            fi
            echo "Uploading Image: $ConfiguratorQcow2ImageName"
            glance image-create --name $ConfiguratorQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $ConfiguratorQcow2Image

        fi

        VyosQcow2ImageName=vyos
        if [[ $VyosQcow2Image = build ]]; then
            echo "Building Image: $VyosQcow2ImageName"
            sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/vyos_conf.json
            VyosQcow2Image=$(cat /tmp/image_path)
        fi
        echo "Uploading Image: $VyosQcow2ImageName"
        glance image-create --name $VyosQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $VyosQcow2Image

        HaproxyQcow2ImageName=haproxy
        if [[ $HaproxyQcow2Image = build ]]; then
            echo "Building Image: $HaproxyQcow2ImageName"
            sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/haproxy_conf.json
            HaproxyQcow2Image=$(cat /tmp/image_path)
        fi
        echo "Uploading Image: $HaproxyQcow2ImageName"
        glance image-create --name $HaproxyQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $HaproxyQcow2Image

    fi
}

function init_nfpgbpservice {
    # Run GBP db migrations
    gbp-db-manage --config-file $NEUTRON_CONF --config-file /$Q_PLUGIN_CONF_FILE upgrade head
    iniset $NEUTRON_CONF DEFAULT policy_dirs $NFP_CONF_DIR
}

function install_nfpgbpservice {
    git_clone $GBPSERVICE_REPO $NFPSERVICE_DIR $GBPSERVICE_BRANCH
    mv $NFPSERVICE_DIR/test-requirements.txt $NFPSERVICE_DIR/_test-requirements.txt
    setup_develop $NFPSERVICE_DIR
    mv -f $NEUTRON_CONF_DIR/policy.json $NEUTRON_CONF_DIR/policy.json.original 2>/dev/null; true
    cp -f $NFPSERVICE_DIR/etc/policy.json $NEUTRON_CONF_DIR/policy.json
    mv $NFPSERVICE_DIR/_test-requirements.txt $NFPSERVICE_DIR/test-requirements.txt
}

function assign_user_role_credential {
    source $TOP_DIR/openrc admin admin
    #set -x
    serviceTenantID=`keystone tenant-list | grep "service" | awk '{print $2}'`
    serviceRoleID=`keystone role-list | grep "service" | awk '{print $2}'`
    adminRoleID=`keystone role-list | grep "admin" | awk '{print $2}'`
    keystone user-role-add --user nova --tenant $serviceTenantID --role $serviceRoleID
    keystone user-role-add --user neutron --tenant $serviceTenantID --role $adminRoleID
}

function namespace_delete {
    source $1/openrc neutron service
    #Deletion namespace
    NFP_P=`sudo ip netns | grep "nfp-proxy"`
    if [ ${#NFP_P} -ne 0 ]; then
        sudo ip netns delete nfp-proxy
        echo "namespace removed"
    fi

    #Delete veth peer
    PEER=`ip a | grep pt1`
    if [ ${#PEER} -ne 0 ]; then
        echo "veth peer removed"
        sudo ip link delete pt1
    fi

    #pt1 port removing from ovs
    PORT=`sudo ovs-vsctl show | grep "pt1"`
    if [ ${#PORT} -ne 0 ]; then
        sudo ovs-vsctl del-port br-int pt1
        echo "ovs port ptr1 is removed"
    fi

    echo "nfp-proxy cleaning success.... "
}

function namespace_create {

    SERVICE_MGMT_NET="l2p_svc_management_ptg"
    cidr="/24"
    echo "Creating new namespace nfp-proxy...."

    #new namespace with name proxy
    NFP_P=`sudo ip netns add nfp-proxy`
    if [ ${#NFP_P} -eq 0 ]; then
        echo "New namepace nfp-proxy created"
    else
        echo "New namespace nfp-proxy creation failed"
        exit 0
    fi

    #Create veth peer
    PEER=`sudo ip link add pt0 type veth peer name pt1`
    if [ ${#PEER} -eq 0 ]; then
        echo "New veth pair created"
    else
        echo "veth pair creation failed"
        exit 0
    fi
    sleep 1

    #move one side of veth into namesape
    sudo ip link set pt0 netns nfp-proxy

    #create new neutron port in service mgmt network
    new_ip=`neutron port-create $SERVICE_MGMT_NET | grep "fixed_ips" | awk '{print $7}' | sed 's/^\"\(.*\)\"}$/\1/'`
    if [ ${#new_ip} -lt 5 ]; then
        echo "new_ip =$new_ip"
        echo "Neutron port creation failed (check source) "
        exit 0
    else
        echo "New Neutron Port Created on Service management network with ip =$new_ip"
    fi
    new_ip_cidr+="$new_ip/24"
    sleep 2

    #get the ip address of new port eg : 11.0.0.6 and asign to namespace
    sudo ip netns exec nfp-proxy ip addr add $new_ip_cidr dev pt0

    #move other side of veth into ovs : br-int
    sudo ovs-vsctl add-port br-int pt1

    #get id of service management network
    smn_id=`neutron net-list | grep "$SERVICE_MGMT_NET" | awk '{print $2}'`

    #get the dhcp namespace of service management network
    nm_space=`sudo ip netns | grep "$smn_id"`

    #get port id from router nampace
    port=`sudo ip netns exec $nm_space ip a | grep "tap" | tail -n 1 | awk '{print $7}'`

    #get tag_id  form port in ovs-bridge
    tag_id=`sudo ovs-vsctl list port $port | grep "tag" | tail -n 1 | awk '{print $3}'`

    sudo ovs-vsctl set port pt1 tag=$tag_id     

    #up the both ports 
    sudo ip netns exec nfp-proxy ip link set pt0 up
    sudo ip netns exec nfp-proxy ip link set lo up
    sudo ip link set pt1 up

    PING=`sudo ip netns exec nfp-proxy ping $2 -q -c 2 > /dev/null`
    if [ ${#PING} -eq 0 ]
    then
        echo "nfp-proxy namespcace creation success and reaching to $2"
    else
        echo "Fails reaching to $2" 
    fi

    sudo ip netns exec nfp-proxy /usr/bin/nfp_proxy --config-file=/etc/nfp_proxy.ini
}


function create_ext_net {

    source $TOP_DIR/stackrc
    EXT_NET_NAME=ext-net
    EXT_NET_SUBNET_NAME=ext-net-subnet
    EXT_NET_GATEWAY=$EXT_NET_GATEWAY
    EXT_NET_ALLOCATION_POOL_START=$EXT_NET_ALLOCATION_POOL_START
    EXT_NET_ALLOCATION_POOL_END=$EXT_NET_ALLOCATION_POOL_END
    EXT_NET_CIDR=$EXT_NET_CIDR
    EXT_NET_MASK=$EXT_NET_MASK
    source $TOP_DIR/openrc neutron service
    unset OS_USER_DOMAIN_ID
    unset OS_PROJECT_DOMAIN_ID

    neutron net-create --router:external=true --shared $EXT_NET_NAME
    neutron subnet-create --ip_version 4 --gateway $EXT_NET_GATEWAY --name $EXT_NET_SUBNET_NAME --allocation-pool start=$EXT_NET_ALLOCATION_POOL_START,end=$EXT_NET_ALLOCATION_POOL_END $EXT_NET_NAME $EXT_NET_CIDR/$EXT_NET_MASK
}


function create_ep_and_nsp {
    
    subnet_id=`neutron net-list | grep "$EXT_NET_NAME" | awk '{print $6}'`
    gbp external-segment-create --ip-version 4 --cidr $EXT_NET_CIDR/$EXT_NET_MASK --external-route destination=0.0.0.0/0,nexthop= --shared True --subnet_id=$subnet_id default
    gbp nat-pool-create --ip-version 4 --ip-pool $EXT_NET_CIDR/$EXT_NET_MASK --external-segment default --shared True default 
    
    gbp ep-create --external-segments default ext_connect    
    gbp nsp-create --network-service-params type=ip_pool,name=vip_ip,value=nat_pool svc_mgmt_fip_policy
}


function create_nfp_gbp_resources {
   
    source $TOP_DIR/openrc neutron service

    if [[ $DEVSTACK_MODE = base ]]; then
        IMAGE_NAME_FLAT="reference_configurator_image"
        FLAVOR=m1.nfp-tiny
        gbp service-profile-create --servicetype LOADBALANCER --insertion-mode l3 --shared True --service-flavor service_vendor=haproxy,device_type=None --vendor NFP base_mode_lb
        gbp service-profile-create --servicetype FIREWALL --insertion-mode l3 --shared True --service-flavor service_vendor=nfp,device_type=nova,image_name=$IMAGE_NAME_FLAT,flavor=$FLAVOR --vendor NFP base_mode_fw_vm
        
        gbp l3policy-create --ip-version 4 --proxy-ip-pool=192.169.0.0/24 --ip-pool 120.0.0.0/24 --subnet-prefix-length 24 service_management
        gbp l2policy-create --l3-policy service_management svc_management_ptg
        
        gbp group-create svc_management_ptg --service_management True --l2-policy svc_management_ptg

    elif [[ $DEVSTACK_MODE = advanced ]]; then
        gbp service-profile-create --servicetype LOADBALANCER --insertion-mode l3 --shared True --service-flavor service_vendor=haproxy,device_type=nova --vendor NFP lb_profile
        gbp service-profile-create --servicetype FIREWALL --insertion-mode l3 --shared True --service-flavor service_vendor=vyos,device_type=nova --vendor NFP fw_profile
        
        gbp l3policy-create --ip-version 4 --proxy-ip-pool=192.169.0.0/24 --ip-pool 120.0.0.0/24 --subnet-prefix-length 24 service_management
        gbp l2policy-create --l3-policy service_management svc_management_ptg
    
        gbp group-create svc_management_ptg --service_management True --l2-policy svc_management_ptg

    else
        gbp service-profile-create --servicetype LOADBALANCER --insertion-mode l3 --shared True --service-flavor service_vendor=haproxy,device_type=nova --vendor NFP lb_profile
        gbp service-profile-create --servicetype FIREWALL --insertion-mode l3 --shared True --service-flavor service_vendor=vyos,device_type=nova --vendor NFP fw_profile
        create_ext_net
        create_ep_and_nsp
        
        gbp l3policy-create --external-segment default --ip-version 4 --proxy-ip-pool=192.169.0.0/24 --ip-pool 120.0.0.0/24 --subnet-prefix-length 24 service_management
        gbp l2policy-create --l3-policy service_management svc_management_ptg

        gbp group-create svc_management_ptg --service_management True --l2-policy svc_management_ptg --network-service-policy svc_mgmt_fip_policy
    fi
}


function copy_nfp_files_and_start_process {

    cd /opt/stack/gbp/gbpservice/nfp
    sudo cp -r  bin/nfp /usr/bin/
    sudo chmod +x /usr/bin/nfp
    sudo rm -rf /etc/nfp_*
    sudo cp -r  bin/nfp_orch_agent.ini /etc/
    sudo cp -r  bin/nfp_proxy_agent.ini /etc/
    sudo cp -r  bin/nfp_config_orch.ini /etc/
    sudo cp -r  bin/proxy.ini /etc/nfp_proxy.ini
    sudo cp -r  bin/nfp_proxy /usr/bin/


    if [[ $DEVSTACK_MODE = base ]]; then
        IpAddr=127.0.0.1
        CONFIGURATOR_PORT=8080
    else
        CONFIGURATOR_PORT=8070
    fi

    echo "Configuring proxy.ini .... with rest_server_address as $IpAddr"
    sudo sed -i "s/rest_server_address=*.*/rest_server_address=$IpAddr/g" /etc/nfp_proxy.ini
    sudo sed -i "s/rest_server_port= *.*/rest_server_port=$CONFIGURATOR_PORT/g" /etc/nfp_proxy.ini

    sed -i 's#source.*#source '$TOP_DIR'/openrc demo demo#g' /opt/stack/gbp/devstack/exercises/nfp_service/*.sh
    source $TOP_DIR/functions-common

    echo "Starting orchestrator  >>>> under screen named : orchestrator"
    run_process orchestrator "sudo /usr/bin/nfp  --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini --config-file /etc/nfp_orch_agent.ini --log-file /opt/stack/logs/nfp_orchestrator.log"
    sleep 1

    echo "Starting proxy_agent  >>>> under screen named : proxy_agent"
    run_process proxy_agent "sudo /usr/bin/nfp --config-file /etc/nfp_proxy_agent.ini --log-file /opt/stack/logs/nfp_proxy_agent.log"
    sleep 1

    echo "Starting proxy server under Namespace : nfp-proxy namespace >>>> under screen named : proxy"
    run_process proxy "source $NFPSERVICE_DIR/devstack/lib/nfp;namespace_delete $TOP_DIR;namespace_create $TOP_DIR $IpAddr"
    sleep 10

    if [[ $DEVSTACK_MODE == advanced ]]; then
        echo "Starting config_orch  >>>> under screen named : config_orch"
        run_process config_orch "sudo /usr/bin/nfp  --config-file /etc/nfp_config_orch.ini --config-file /etc/neutron/neutron.conf --log-file /opt/stack/logs/nfp_config_orch.log"
    else
        cd base_configurator/api
        sudo python setup.py develop
        echo "Starting base_configurator  >>>> under screen named : base_configurator"
        run_process base_configurator "cd /opt/stack/gbp/gbpservice/nfp/base_configurator/api;sudo ip netns exec nfp-proxy pecan serve config.py"
    fi
    sleep 1

    echo "Running gbp-db-manage"

    source $TOP_DIR/openrc neutron service

    gbp-db-manage --config-file /etc/neutron/neutron.conf upgrade head
    sleep 2
    echo "NFP configuration done...!! "

}

function launch_configuratorVM {
    InstanceName="configuratorVM_instance"

    GROUP="svc_management_ptg"
    echo "GroupName: $GROUP"
    PortId=$(gbp policy-target-create --policy-target-group $GROUP $InstanceName | grep port_id  | awk '{print $4}')

    ConfiguratorImageName=configurator
    echo "Collecting ImageId : for $ConfiguratorImageName"
    ImageId=`glance image-list|grep $ConfiguratorImageName |awk '{print $2}'`
    if [ ! -z "$ImageId" -a "$ImageId" != " " ]; then
        echo $ImageId
    else
        echo "No image found with name $ConfiguratorImageName ..."
        exit
    fi

    nova boot --flavor m1.medium --image $ImageId --nic port-id=$PortId $InstanceName
    sleep 10

    echo "Get IpAddr with port: $PortId"
    IpAddr_extractor=`neutron port-list|grep $PortId|awk '{print $11}'`
    IpAddr_purge_last=${IpAddr_extractor::-1}
    IpAddr=${IpAddr_purge_last//\"/}
    echo "Collecting IpAddr : for $PortId"
    echo $IpAddr
}

function nfp_logs_forword {
    VISIBILITY_CONF="/etc/rsyslog.d/visibility.conf"
    SYSLOG_CONFIG="/etc/rsyslog.conf"
    log_facility=local1

    sudo sed -i '/#$ModLoad imudp/ s/^#//' $SYSLOG_CONFIG
    sudo sed -i '/#$UDPServerRun 514/ s/^#//' $SYSLOG_CONFIG
    echo "Successfully enabled UDP in syslog"

    visibility_vm_ip_address=`neutron floatingip-list | grep "$IpAddr" | awk '{print $6}'`
    echo "$log_facility.*                @$visibility_vm_ip_address:514" | sudo tee -a $VISIBILITY_CONF
    echo "Created $VISIBILITY_CONF file" 

    sudo service rsyslog restart
    if [ $? -ne 0 ]; then
        echo "ERROR: Failed to restart rsyslog"
    fi 

}


function configure_visibility_user_data {
 
    CUR_DIR=$PWD
    sudo rm -rf /opt/visibility_user_data
    sudo cp -r $TOP_DIR/gbp/devstack/exercises/nfp_service/user-data/visibility_user_data /opt/.
    cd /opt
    sudo rm -rf my.key my.key.pub
    visibility_vm_ip=$1
    sudo ssh-keygen -t rsa -N "" -f my.key
    value=`sudo cat my.key.pub`
    sudo echo  $value
    sudo sed -i "8 i\      -\ $value" visibility_user_data
    sudo sed -i '9d' visibility_user_data
    sudo sed -i "s/visibility_vm_ip=*.*/visibility_vm_ip=$visibility_vm_ip/g" visibility_user_data
    sudo sed -i "s/sc_active_ip=*.*/sc_active_ip=$HOST_IP/g" visibility_user_data
    sudo sed -i "s/sc_standby_ip=*.*/sc_standby_ip=$HOST_IP/g" visibility_user_data
    sudo sed -i "s/service_controller_vip=*.*/service_controller_vip=$HOST_IP:8080/g" visibility_user_data
    sudo sed -i "s/db_user=*.*/db_user=root/g" visibility_user_data
    sudo sed -i "s/db_password=*.*/db_password=$MYSQL_PASSWORD/g" visibility_user_data
    sudo sed -i "s/db_server_ip=*.*/db_server_ip=$HOST_IP/g" visibility_user_data
    sudo sed -i "s/statsd_host=*.*/db_server_ip=$visibility_vm_ip/g" visibility_user_data
    sudo sed -i "s/statsd_port=*.*/statsd_port=8125/g" visibility_user_data
    sudo sed -i "s/rsyslog_host=*.*/rsyslog_host=$visibility_vm_ip/g" visibility_user_data
    sudo sed -i "s/rabbit_host=*.*/rabbit_host=$visibility_vm_ip/g" visibility_user_data
    sudo sed -i "s/rabbit_port=*.*/rabbit_port=5672/g" visibility_user_data
    sudo sed -i "s/rsyslog_host=*.*/rsyslog_host=$visibility_vm_ip/g" visibility_user_data
    cd $CUR_DIR
}


function launch_visibilityVM {

    
    VisibilityImageName=visibility
    
    InstanceName="VisibilityVM_instance"
    GROUP="svc_management_ptg"
    echo "GroupName: $GROUP"
    PortId=$(gbp policy-target-create --policy-target-group $GROUP $InstanceName | grep port_id  | awk '{print $4}')
    echo "Collecting ImageId : for $VisibilityImageName"
    VisibilityVMImageId=`glance image-list|grep $VisibilityImageName |awk '{print $2}'`
    if [ ! -z "$VisibilityVMImageId" -a "$VisibilityVMImageId" != " " ]; then
        echo $VisibilityVMImageId
    else
        echo "No image found with name $VisibilityImageName ..."
        exit
    fi

    echo "Get IpAddr with port: $PortId"
    IpAddr_extractor=`neutron port-list|grep $PortId|awk '{print $11}'`
    IpAddr_purge_last=${IpAddr_extractor::-1}
    IpAddr=${IpAddr_purge_last//\"/}
    echo "Collecting IpAddr : for $PortId"
    echo $IpAddr

    configure_visibility_user_data $IpAddr
    
    echo "Launching Visibility image"
    nova boot --image  $VisibilityVMImageId --flavor m1.xlarge --user-data /opt/visibility_user_data  --nic port-id=$PortId $InstanceName 
    sleep 10 
}
