#!/bin/bash

NFPSERVICE_DIR=$DEST/gbp
TOP_DIR=$PWD
DISK_IMAGE_DIR=$DEST/gbp/gbpservice/tests/contrib

NEUTRON_CONF_DIR=/etc/neutron
NEUTRON_CONF=$NEUTRON_CONF_DIR/neutron.conf

NFP_CONF_DIR=/etc/nfp
echo "TOP-DIR-NFP : $PWD"

function prepare_nfp_image_builder {
    sudo -H -E pip install -r $DISK_IMAGE_DIR/diskimage-create/requirements.txt
    sudo apt-get install -y --force-yes qemu-utils
    sudo apt-get install -y --force-yes dpkg-dev
    if [[ $DEVSTACK_MODE != base ]]; then
        sudo wget -qO- https://get.docker.com/ | bash
    fi
}


function configure_vis_ip_addr_in_docker {
    
    echo "Visibility VM IP address is: $visibility_ip"
    sed -i "s/VIS_VM_IP_ADDRESS/"$visibility_ip"/" $NFPSERVICE_DIR/gbpservice/nfp/configurator/Dockerfile

}


function create_nfp_image {
    source $TOP_DIR/openrc neutron service
    unset OS_USER_DOMAIN_ID
    unset OS_PROJECT_DOMAIN_ID

    # during diskimage build, the following setting in apache2 is needed for local repo
    sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-enabled/
    sudo service apache2 restart

    if [[ $DEVSTACK_MODE = base ]]; then

        RefConfiguratorQcow2ImageName=reference_configurator_image
        echo "Building Image: $RefConfiguratorQcow2ImageName"
        sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/ref_configurator_conf.json
        RefConfiguratorQcow2Image=$(cat /tmp/image_path)
        echo "Uploading Image: $RefConfiguratorQcow2ImageName"
        glance image-create --name $RefConfiguratorQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $RefConfiguratorQcow2Image
        openstack --os-cloud=devstack-admin flavor create --ram 512 --disk 3 --vcpus 1 m1.nfp-tiny

    else

        if [[ $DEVSTACK_MODE = enterprise ]]; then

            ConfiguratorQcow2ImageName=configurator
            ConfiguratorInstanceName="configuratorVM_instance"
            create_port_for_vm $ConfiguratorQcow2ImageName
            
            if [[ $ConfiguratorQcow2Image = build ]]; then
                echo "Building Image: $ConfiguratorQcow2ImageName"
                sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/configurator_conf.json $TOP_DIR/local.conf
                ConfiguratorQcow2Image=$(cat /tmp/image_path)
            fi
            echo "Uploading Image: $ConfiguratorQcow2ImageName"
            glance image-create --name $ConfiguratorQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $ConfiguratorQcow2Image

            VisibilityQcow2ImageName=visibility
            VisibilityInstanceName="VisibilityVM_instance"
            create_port_for_vm $VisibilityQcow2ImageName

            if [[ $VisibilityQcow2Image = build ]]; then

                # edits the docker file to add visibility vm IP address
                configure_vis_ip_addr_in_docker

                # prepare visibility source, this is needed for diskimage build
                cd /home/stack/
                sudo rm -rf visibility
                sudo git clone https://$GIT_ACCESS_USERNAME:$GIT_ACCESS_PASSWORD@github.com/oneconvergence/visibility.git -b $VISIBILITY_GIT_BRANCH
                echo "Building Image: $VisibilityQcow2ImageName"
                sudo python $DISK_IMAGE_DIR/diskimage-create/visibility_disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/visibility_conf.json $TOP_DIR/local.conf
                VisibilityQcow2Image=$(cat /tmp/image_path)
            fi
            echo "Uploading Image: $VisibilityQcow2ImageName"
            glance image-create --name $VisibilityQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $VisibilityQcow2Image

            AsavQcow2ImageName=asav
            echo "Uploading Image: $AsavQcow2ImageName"
            glance image-create --name $AsavQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $AsavQcow2Image
            PaloAltoQcow2ImageName=paloalto
            echo "Uploading Image: $PaloAltoQcow2ImageName"
            glance image-create --name $PaloAltoQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $PaloAltoQcow2Image

        else

            ConfiguratorQcow2ImageName=configurator
            InstanceName="configuratorVM_instance"
            create_port_for_vm $ConfiguratorQcow2ImageName
            
            if [[ $ConfiguratorQcow2Image = build ]]; then
                echo "Building Image: $ConfiguratorQcow2ImageName"
                sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/configurator_conf.json $TOP_DIR/local.conf
                ConfiguratorQcow2Image=$(cat /tmp/image_path)
            fi
            echo "Uploading Image: $ConfiguratorQcow2ImageName"
            glance image-create --name $ConfiguratorQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $ConfiguratorQcow2Image

        fi

        VyosQcow2ImageName=vyos
        if [[ $VyosQcow2Image = build ]]; then
            echo "Building Image: $VyosQcow2ImageName"
            sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/vyos_conf.json
            VyosQcow2Image=$(cat /tmp/image_path)
        fi
        echo "Uploading Image: $VyosQcow2ImageName"
        glance image-create --name $VyosQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $VyosQcow2Image

        HaproxyQcow2ImageName=haproxy
        if [[ $HaproxyQcow2Image = build ]]; then
            echo "Building Image: $HaproxyQcow2ImageName"
            sudo python $DISK_IMAGE_DIR/diskimage-create/disk_image_create.py $DISK_IMAGE_DIR/diskimage-create/haproxy_conf.json
            HaproxyQcow2Image=$(cat /tmp/image_path)
        fi
        echo "Uploading Image: $HaproxyQcow2ImageName"
        glance image-create --name $HaproxyQcow2ImageName --disk-format qcow2 --container-format bare --visibility public --file $HaproxyQcow2Image
        Haproxy_LBaasV2_Qcow2Image_Name=haproxy_lbaasv2
        echo "Uploading Image: $Haproxy_LBaasV2_Qcow2Image_Name"
        glance image-create --name $Haproxy_LBaasV2_Qcow2Image_Name --disk-format qcow2 --container-format bare --visibility public --file $Haproxy_LBaasV2_Qcow2Image

    fi

    # restore the apache2 setting that we did above
    sudo rm /etc/apache2/sites-enabled/000-default.conf
    sudo service apache2 restart
}

function init_nfpgbpservice {
    # Run GBP db migrations
    gbp-db-manage --config-file $NEUTRON_CONF --config-file /$Q_PLUGIN_CONF_FILE upgrade head
    iniset $NEUTRON_CONF DEFAULT policy_dirs $NFP_CONF_DIR
}

function install_nfpgbpservice {
    git_clone $GBPSERVICE_REPO $NFPSERVICE_DIR $GBPSERVICE_BRANCH
    mv $NFPSERVICE_DIR/test-requirements.txt $NFPSERVICE_DIR/_test-requirements.txt
    setup_develop $NFPSERVICE_DIR
    mv -f $NEUTRON_CONF_DIR/policy.json $NEUTRON_CONF_DIR/policy.json.original 2>/dev/null; true
    cp -f $NFPSERVICE_DIR/etc/policy.json $NEUTRON_CONF_DIR/policy.json
    mv $NFPSERVICE_DIR/_test-requirements.txt $NFPSERVICE_DIR/test-requirements.txt
}

function assign_user_role_credential {
    source $TOP_DIR/openrc admin admin
    #set -x
    serviceTenantID=`keystone tenant-list | grep "service" | awk '{print $2}'`
    serviceRoleID=`keystone role-list | grep "service" | awk '{print $2}'`
    adminRoleID=`keystone role-list | grep "admin" | awk '{print $2}'`
    keystone user-role-add --user nova --tenant $serviceTenantID --role $serviceRoleID
    keystone user-role-add --user neutron --tenant $serviceTenantID --role $adminRoleID
}

function namespace_delete {
    source $1/openrc neutron service
    #Deletion namespace
    NFP_P=`sudo ip netns | grep "nfp-proxy"`
    if [ ${#NFP_P} -ne 0 ]; then
        sudo ip netns delete nfp-proxy
        echo "namespace removed"
    fi

    #Delete veth peer
    PEER=`ip a | grep pt1`
    if [ ${#PEER} -ne 0 ]; then
        echo "veth peer removed"
        sudo ip link delete pt1
    fi

    #pt1 port removing from ovs
    PORT=`sudo ovs-vsctl show | grep "pt1"`
    if [ ${#PORT} -ne 0 ]; then
        sudo ovs-vsctl del-port br-int pt1
        echo "ovs port ptr1 is removed"
    fi

    echo "nfp-proxy cleaning success.... "
}

function namespace_create {

    SERVICE_MGMT_NET="l2p_svc_management_ptg"
    cidr="/24"
    echo "Creating new namespace nfp-proxy...."

    #new namespace with name proxy
    NFP_P=`sudo ip netns add nfp-proxy`
    if [ ${#NFP_P} -eq 0 ]; then
        echo "New namepace nfp-proxy created"
    else
        echo "New namespace nfp-proxy creation failed"
        exit 0
    fi

    #Create veth peer
    PEER=`sudo ip link add pt0 type veth peer name pt1`
    if [ ${#PEER} -eq 0 ]; then
        echo "New veth pair created"
    else
        echo "veth pair creation failed"
        exit 0
    fi
    sleep 1

    #move one side of veth into namesape
    sudo ip link set pt0 netns nfp-proxy

    #create new neutron port in service mgmt network
    new_ip=`neutron port-create $SERVICE_MGMT_NET | grep "fixed_ips" | awk '{print $7}' | sed 's/^\"\(.*\)\"}$/\1/'`
    if [ ${#new_ip} -lt 5 ]; then
        echo "new_ip =$new_ip"
        echo "Neutron port creation failed (check source) "
        exit 0
    else
        echo "New Neutron Port Created on Service management network with ip =$new_ip"
    fi
    new_ip_cidr+="$new_ip/24"
    sleep 2

    #get the ip address of new port eg : 11.0.0.6 and asign to namespace
    sudo ip netns exec nfp-proxy ip addr add $new_ip_cidr dev pt0

    #move other side of veth into ovs : br-int
    sudo ovs-vsctl add-port br-int pt1

    #get id of service management network
    smn_id=`neutron net-list | grep "$SERVICE_MGMT_NET" | awk '{print $2}'`

    #get the dhcp namespace of service management network
    nm_space=`sudo ip netns | grep "$smn_id"`

    #get port id from router nampace
    port=`sudo ip netns exec $nm_space ip a | grep "tap" | tail -n 1 | awk '{print $7}'`

    #get tag_id  form port in ovs-bridge
    tag_id=`sudo ovs-vsctl list port $port | grep "tag" | tail -n 1 | awk '{print $3}'`

    sudo ovs-vsctl set port pt1 tag=$tag_id     

    #up the both ports 
    sudo ip netns exec nfp-proxy ip link set pt0 up
    sudo ip netns exec nfp-proxy ip link set lo up
    sudo ip link set pt1 up

    PING=`sudo ip netns exec nfp-proxy ping $2 -q -c 2 > /dev/null`
    if [ ${#PING} -eq 0 ]
    then
        echo "nfp-proxy namespcace creation success and reaching to $2"
    else
        echo "Fails reaching to $2" 
    fi

    sudo ip netns exec nfp-proxy /usr/bin/nfp_proxy --config-file=/etc/nfp_proxy.ini
}


function create_ext_net {

    source $TOP_DIR/stackrc
    EXT_NET_NAME=ext-net
    EXT_NET_SUBNET_NAME=ext-net-subnet
    EXT_NET_GATEWAY=$EXT_NET_GATEWAY
    EXT_NET_ALLOCATION_POOL_START=$EXT_NET_ALLOCATION_POOL_START
    EXT_NET_ALLOCATION_POOL_END=$EXT_NET_ALLOCATION_POOL_END
    EXT_NET_CIDR=$EXT_NET_CIDR
    EXT_NET_MASK=$EXT_NET_MASK
    source $TOP_DIR/openrc neutron service
    unset OS_USER_DOMAIN_ID
    unset OS_PROJECT_DOMAIN_ID

    neutron net-create --router:external=true --shared $EXT_NET_NAME
    neutron subnet-create --ip_version 4 --gateway $EXT_NET_GATEWAY --name $EXT_NET_SUBNET_NAME --allocation-pool start=$EXT_NET_ALLOCATION_POOL_START,end=$EXT_NET_ALLOCATION_POOL_END $EXT_NET_NAME $EXT_NET_CIDR/$EXT_NET_MASK
}


function create_ep_and_nsp {
    
    subnet_id=`neutron net-list | grep "$EXT_NET_NAME" | awk '{print $6}'`
    gbp external-segment-create --ip-version 4 --cidr $EXT_NET_CIDR/$EXT_NET_MASK --external-route destination=0.0.0.0/0,nexthop= --shared True --subnet_id=$subnet_id default
    gbp nat-pool-create --ip-version 4 --ip-pool $EXT_NET_CIDR/$EXT_NET_MASK --external-segment default --shared True default 
    
    gbp ep-create --external-segments default ext_connect    
    gbp nsp-create --network-service-params type=ip_pool,name=vip_ip,value=nat_pool svc_mgmt_fip_policy
}


function create_advance_sharing_ptg {

        gbp l3policy-create --ip-version 4 --ip-pool 121.0.0.0/20 --proxy-ip-pool=192.167.0.0/24 --subnet-prefix-length 20 advanced_services_sharing_l3p
        gbp l2policy-create --l3-policy advanced_services_sharing_l3p advance_sharing_l2p
        gbp group-create --l2-policy advance_sharing_l2p Advance_Sharing_PTG
}


function create_nfp_gbp_resources {
   
    source $TOP_DIR/openrc neutron service

    if [[ $DEVSTACK_MODE = base ]]; then
        IMAGE_NAME_FLAT="reference_configurator_image"
        FLAVOR=m1.nfp-tiny
        gbp service-profile-create --servicetype LOADBALANCER --insertion-mode l3 --shared True --service-flavor service_vendor=haproxy,device_type=None --vendor NFP base_mode_lb
        gbp service-profile-create --servicetype FIREWALL --insertion-mode l3 --shared True --service-flavor service_vendor=nfp,device_type=nova,image_name=$IMAGE_NAME_FLAT,flavor=$FLAVOR --vendor NFP base_mode_fw_vm
    else
        gbp service-profile-create --servicetype LOADBALANCER --insertion-mode l3 --shared True --service-flavor service_vendor=haproxy,device_type=nova --vendor NFP lb_profile
        gbp service-profile-create --shared True --vendor NFP --servicetype LOADBALANCERV2 --service-flavor service_vendor=haproxy_lbaasv2,device_type=nova,flavor=m1.small --insertion-mode l3 lbv2_profile
        gbp service-profile-create --servicetype FIREWALL --insertion-mode l3 --shared True --service-flavor service_vendor=vyos,device_type=nova --vendor NFP vyos_fw_profile
        gbp service-profile-create --servicetype VPN --insertion-mode l3 --shared True --service-flavor service_vendor=vyos,device_type=nova --vendor NFP vpn_profile

        if [[ $DEVSTACK_MODE = enterprise ]]; then
            gbp service-profile-create --servicetype FIREWALL --insertion-mode l3 --shared True --service-flavor service_vendor=asav,device_type=nova --vendor NFP asav_fw_profile
        fi  
        create_ext_net
        create_ep_and_nsp
        create_advance_sharing_ptg
    fi

    gbp l3policy-create --ip-version 4 --proxy-ip-pool=192.169.0.0/24 --ip-pool 120.0.0.0/24 --subnet-prefix-length 24 service_management
    gbp l2policy-create --l3-policy service_management svc_management_ptg

    gbp group-create svc_management_ptg --service_management True --l2-policy svc_management_ptg
    neutron router-gateway-clear l3p_service_management
}


function copy_nfp_files_and_start_process {

    cd /opt/stack/gbp/gbpservice/nfp
    sudo cp -r  bin/nfp /usr/bin/
    sudo chmod +x /usr/bin/nfp
    sudo rm -rf /etc/nfp_*
    sudo cp -r  bin/nfp_orch_agent.ini /etc/
    sudo cp -r  bin/nfp_proxy_agent.ini /etc/
    [[ $DEVSTACK_MODE != base ]] && sudo cp -r  bin/nfp_config_orch.ini /etc/
    sudo cp -r  bin/proxy.ini /etc/nfp_proxy.ini
    sudo cp -r  bin/nfp_proxy /usr/bin/


    if [[ $DEVSTACK_MODE = base ]]; then
        IpAddr=127.0.0.1
        CONFIGURATOR_PORT=8080
    else
        CONFIGURATOR_PORT=8070
        IpAddr=$configurator_ip
    fi

    echo "Configuring proxy.ini .... with rest_server_address as $IpAddr"
    sudo sed -i "s/rest_server_address=*.*/rest_server_address=$IpAddr/g" /etc/nfp_proxy.ini
    sudo sed -i "s/rest_server_port= *.*/rest_server_port=$CONFIGURATOR_PORT/g" /etc/nfp_proxy.ini

    sed -i 's#source.*#source '$TOP_DIR'/openrc demo demo#g' /opt/stack/gbp/devstack/exercises/nfp_service/*.sh
    source $TOP_DIR/functions-common

    echo "Starting orchestrator  >>>> under screen named : orchestrator"
    run_process orchestrator "sudo /usr/bin/nfp  --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini --config-file /etc/nfp_orch_agent.ini --log-file /opt/stack/logs/nfp_orchestrator.log"
    sleep 4

    echo "Starting proxy_agent  >>>> under screen named : proxy_agent"
    run_process proxy_agent "sudo /usr/bin/nfp --config-file /etc/nfp_proxy_agent.ini --log-file /opt/stack/logs/nfp_proxy_agent.log"
    sleep 4

    echo "Starting proxy server under Namespace : nfp-proxy namespace >>>> under screen named : proxy"
    run_process proxy "source $NFPSERVICE_DIR/devstack/lib/nfp;namespace_delete $TOP_DIR;namespace_create $TOP_DIR $IpAddr"
    sleep 10

    if [[ $DEVSTACK_MODE != base ]]; then
        echo "Starting config_orch  >>>> under screen named : config_orch"
        run_process config_orch "sudo /usr/bin/nfp  --config-file /etc/nfp_config_orch.ini --config-file /etc/neutron/neutron.conf --log-file /opt/stack/logs/nfp_config_orch.log"
    else
        cd base_configurator/api
        sudo python setup.py develop
        echo "Starting base_configurator  >>>> under screen named : base_configurator"
        run_process base_configurator "cd /opt/stack/gbp/gbpservice/nfp/base_configurator/api;sudo ip netns exec nfp-proxy pecan serve config.py"
    fi
    sleep 1

    echo "Running gbp-db-manage"

    source $TOP_DIR/openrc neutron service

    gbp-db-manage --config-file /etc/neutron/neutron.conf upgrade head
    sleep 2
    echo "NFP configuration done...!! "

}


function nfp_logs_forword {
    VISIBILITY_CONF="/etc/rsyslog.d/visibility.conf"
    SYSLOG_CONFIG="/etc/rsyslog.conf"
    log_facility=local1

    sudo sed -i '/#$ModLoad imudp/ s/^#//' $SYSLOG_CONFIG
    sudo sed -i '/#$UDPServerRun 514/ s/^#//' $SYSLOG_CONFIG
    echo "Successfully enabled UDP in syslog"

    visibility_vm_ip_address=$(neutron floatingip-list --format value | grep "$IpAddr2" |  awk '{print $3}')
    echo "$log_facility.* @$visibility_vm_ip_address:514" | sudo tee $VISIBILITY_CONF
    echo "Created $VISIBILITY_CONF file" 

    sudo service rsyslog restart
    if [ $? -ne 0 ]; then
        echo "ERROR: Failed to restart rsyslog"
    fi 

}


function configure_configurator_user_data {

    CUR_DIR=$PWD
    sudo rm -rf /opt/configurator_user_data
    sudo cp -r $DEST/gbp/devstack/exercises/nfp_service/user-data/configurator_user_data /opt/.
    cd /opt
    sudo rm -rf my.key my.key.pub
    sudo ssh-keygen -t rsa -N "" -f my.key
    value=`sudo cat my.key.pub`
    sudo echo  $value
    sudo sed -i "8 i\      -\ $value" configurator_user_data
    sudo sed -i '9d' configurator_user_data
    cd $CUR_DIR
}

function configure_visibility_user_data {
 
    CUR_DIR=$PWD
    visibility_vm_ip=$1

    sudo rm -rf /opt/visibility_user_data
    sudo cp -r $DEST/gbp/devstack/exercises/nfp_service/user-data/visibility_user_data /opt/.
    cd /opt

    sudo rm -rf my.key my.key.pub
    sudo ssh-keygen -t rsa -N "" -f my.key
    value=`sudo cat my.key.pub`
    sudo echo  $value
    sudo sed -i "s|<SSH PUBLIC KEY>|${value}|" visibility_user_data

    sudo sed -i "s/visibility_vm_ip=*.*/visibility_vm_ip=$visibility_vm_ip/g" visibility_user_data
    sudo sed -i "s/os_controller_ip=*.*/os_controller_ip=$HOST_IP/g" visibility_user_data
    sudo sed -i "s/statsd_host=*.*/statsd_host=$visibility_vm_ip/g" visibility_user_data
    sudo sed -i "s/rabbit_host=*.*/rabbit_host=$configurator_ip/g" visibility_user_data

    cd $CUR_DIR
}


function attach_security_groups {

    unset OS_USER_DOMAIN_ID
    unset OS_PROJECT_DOMAIN_ID

    SecGroup="allow_all"
    nova secgroup-create $SecGroup "allow all traffic"
    nova secgroup-add-rule $SecGroup udp 1 65535  120.0.0.0/24
    nova secgroup-add-rule $SecGroup icmp -1 -1   120.0.0.0/24
    nova secgroup-add-rule $SecGroup tcp 1 65535  120.0.0.0/24
    nova secgroup-add-rule $SecGroup tcp 80 80  188.0.0.0/24
    nova secgroup-add-rule $SecGroup udp  514 514  188.0.0.0/24
}


function launch_configuratorVM {

    echo "Collecting ImageId : for $configurator_image_name"
    ImageId=`glance image-list|grep $configurator_image_name |awk '{print $2}'`
    if [ ! -z "$ImageId" -a "$ImageId" != " " ]; then
        echo $ImageId
    else
        echo "No image found with name $configurator_image_name ..."
        exit
    fi

    configure_configurator_user_data
    nova boot --flavor m1.medium --user-data /opt/configurator_user_data --image $ImageId --nic port-id=$configurator_port_id $ConfiguratorInstanceName

    sleep 10

}


function create_port_for_vm {

    if [[ $1 = configurator ]]; then
        instance_name=$ConfiguratorInstanceName
    else
        instance_name=$VisibilityInstanceName
    fi

    GROUP="svc_management_ptg"
    echo "GroupName: $GROUP"
    PortId=$(gbp policy-target-create --policy-target-group $GROUP $instance_name | grep port_id  | awk '{print $4}')

    echo "Get IpAddr with port: $PortId"
    IpAddr_extractor=`neutron port-list --format value|grep $PortId|awk '{print $7}'`
    IpAddr_purge_last=${IpAddr_extractor::-1}
    IpAddr=${IpAddr_purge_last//\"/}
    echo "Collecting IpAddr : for $PortId"
    echo $IpAddr

    if [[ $1 = configurator ]]; then
        configurator_image_name=$1
        configurator_port_id=$PortId
        configurator_ip=$IpAddr
    else
        visibility_image_name=$1
        visibility_port_id=$PortId
        visibility_ip=$IpAddr
    fi
}


function launch_visibilityVM {
 
    neutron net-create visibility-network
    neutron subnet-create visibility-network  188.0.0.0/24 --name visibility-subnet
    neutron router-create visibility-router
    neutron router-gateway-set visibility-router $EXT_NET_NAME
    neutron router-interface-add visibility-router visibility-subnet
    ExtPortId=$(neutron port-create visibility-network |  grep ' id    ' | awk '{print $4}')

    fip_id=$(neutron floatingip-create $EXT_NET_NAME | grep ' id '| awk '{print $4}')
    neutron floatingip-associate $fip_id $ExtPortId
    
    IpAddr_extractor=`neutron port-list --format value|grep $ExtPortId|awk '{print $6}'`
    IpAddr_purge_last=${IpAddr_extractor::-1}
    IpAddr2=${IpAddr_purge_last//\"/}
    echo "Collecting IpAddr : for $ExtPortId"
    echo $IpAddr2
    
    configure_visibility_user_data $visibility_ip

    echo "Collecting ImageId : for $visibility_image_name"
    ImageId=`glance image-list|grep $visibility_image_name |awk '{print $2}'`
    if [ ! -z "$ImageId" -a "$ImageId" != " " ]; then
        echo $ImageId
    else
        echo "No image found with name $visibility_image_name ..."
        exit
    fi

    attach_security_groups
    echo "Launching Visibility image"
    nova boot --image  $ImageId --flavor m1.xlarge --user-data /opt/visibility_user_data  --nic port-id=$visibility_port_id --nic port-id=$ExtPortId $VisibilityInstanceName 

    sleep 10
    nova add-secgroup $InstanceName $SecGroup
}
